# -*- coding: utf-8 -*-
"""UASDSLAST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hCeFZmrWK1xVCXaLUwcivpBDptbM-e04
"""

pip install ucimlrepo

# SETUP & IMPORTS
import os
import joblib  # Untuk menyimpan model
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Library Machine Learning
from ucimlrepo import fetch_ucirepo
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.utils import class_weight
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping

# Membuat folder untuk menyimpan hasil (Artifacts)
os.makedirs('models', exist_ok=True)
os.makedirs('images', exist_ok=True)
os.makedirs('data', exist_ok=True)

# DATA LOADING & CLEANING
print("Sedang mengambil dataset dari UCI Repository...")
from ucimlrepo import fetch_ucirepo
toxicity = fetch_ucirepo(id=728)

# Mengambil fitur dan target
X_raw = toxicity.data.features
y_raw = toxicity.data.targets

# Menggabungkan Fitur dan Target menjadi satu DataFrame
raw_dataset = pd.concat([X_raw, y_raw], axis=1)

# Simpan ke file
raw_dataset.to_csv('data/toxicity_raw_data.csv', index=False)
print("Berhasil! File tersimpan sebagai 'toxicity_raw_data.csv'.")

print(f"Dataset Asli: {X_raw.shape[0]} Baris, {X_raw.shape[1]} Fitur")

print("\n[Tabel] 5 Baris Pertama Data MENTAH (Sebelum Cleaning):")
display(X_raw.head()) # Menampilkan tabel awal

# Cek Missing Values & Cleaning
print("\n--- Proses Cleaning ---")
missing_count = X_raw.isnull().sum().sum()
if missing_count > 0:
    print(f"Ditemukan {missing_count} missing values.")
    print("Melakukan imputasi (mengisi nilai kosong) dengan rata-rata...")
    X_raw.fillna(X_raw.mean(), inplace=True)
    print("Imputasi selesai.")
else:
    print("Data sudah bersih (Tidak ada missing values).")

# --- MENAMPILKAN DATA SETELAH CLEANING ---
print("\n[Tabel] 5 Baris Pertama Data BERSIH (Setelah Cleaning):")
display(X_raw.head()) # Menampilkan tabel setelah dibersihkan

# Encoding Target (Mengubah 'Toxic'/'NonToxic' menjadi 1/0)
print("\n--- Encoding Target ---")
le = LabelEncoder()
# Mengambil kolom pertama dari y_raw karena y_raw adalah DataFrame
y_encoded = le.fit_transform(y_raw.iloc[:, 0])
y = pd.Series(y_encoded, name='Target')

# Simpan mapping kelas untuk referensi nanti
class_names = le.classes_
print(f"Mapping Kelas: {class_names} -> [0, 1]")

# FEATURE ENGINEERING
print("Melakukan Seleksi Fitur...")

selector = SelectKBest(score_func=f_classif, k=30)
X_selected = selector.fit_transform(X_raw, y)

# Mendapatkan nama fitur yang terpilih
mask = selector.get_support()
selected_features = X_raw.columns[mask]

# Membuat DataFrame baru dengan fitur terpilih
X_final = pd.DataFrame(X_selected, columns=selected_features)

print(f"Dimensi Awal: {X_raw.shape}")
print(f"Dimensi Setelah Seleksi: {X_final.shape}")
print(f"Fitur Terpilih: {list(selected_features[:5])} ...") # Print 5 fitur pertama

# DATA TRANSFORMATION
scaler = StandardScaler()

# Melakukan scaling pada X_final
X_scaled_array = scaler.fit_transform(X_final)
X_scaled = pd.DataFrame(X_scaled_array, columns=selected_features)

print("Beberapa data setelah scaling:")
print(X_scaled.iloc[:3, :5]) # Print 3 baris pertama, 5 kolom pertama

# SIMPAN PROCESSED DATA

processed_dataset = pd.concat([X_scaled, y], axis=1)

processed_dataset.to_csv('data/toxicity_processed_data.csv', index=False)
print("Berhasil! File 'toxicity_processed_data.csv' siap digunakan untuk modelling.")

processed_dataset.head()

# DATA SPLITTING
# Target: Train=70%, Val=15%, Test=15%

# Langkah 1: Pisahkan Test (15%) dari total data
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_scaled, y, test_size=0.15, random_state=42, stratify=y
)

# Langkah 2: Pisahkan Train (70%) dan Val (15%) dari sisa data
# Hitungan: 0.15 / 0.85 (sisa) ~= 0.176
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val
)

print(f"Total Data: {len(X_scaled)}")
print(f"Train Set : {X_train.shape[0]} ({X_train.shape[0]/len(X_scaled):.1%})")
print(f"Val Set   : {X_val.shape[0]} ({X_val.shape[0]/len(X_scaled):.1%})")
print(f"Test Set  : {X_test.shape[0]} ({X_test.shape[0]/len(X_scaled):.1%})")
print("✅ Data Splitting Selesai.")

# DATA BALANCING
# Kita gunakan teknik Class Weighting (bukan oversampling) untuk menjaga keaslian data
weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = {i: weights[i] for i in range(len(weights))}

print(f"Class Weights: {class_weight_dict}")

# MODEL 1 - BASELINE (Logistic Regression)
print("Training Baseline Model...")
model_baseline = LogisticRegression(random_state=42, class_weight='balanced')
model_baseline.fit(X_train, y_train)

# Simpan Model
joblib.dump(model_baseline, 'models/model_baseline.pkl')
print("✅ Baseline Model Trained & Saved.")

# MODEL 2 - ADVANCED (Random Forest)
print("Training Advanced Model...")
# Menggunakan n_estimators=100 dan max_depth dibatasi agar tidak overfitting di data kecil
model_rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    class_weight='balanced'
)
model_rf.fit(X_train, y_train)

# Simpan Model
joblib.dump(model_rf, 'models/model_advanced.pkl')
print("✅ Advanced Model Trained & Saved.")

# MODEL 3 - DEEP LEARNING (MLP)
print("Training Deep Learning Model...")

# Arsitektur Model
# Karena data sedikit, kita gunakan Dropout yang cukup besar untuk mencegah overfitting
model_dl = Sequential([
    Input(shape=(X_train.shape[1],)),       # Input Layer (30 fitur)
    Dense(64, activation='relu'),           # Hidden Layer 1
    Dropout(0.5),
    Dense(32, activation='relu'),           # Hidden Layer 2
    Dropout(0.3),
    Dense(1, activation='sigmoid')          # Output Layer (Binary)
])

model_dl.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Training dengan Early Stopping
history = model_dl.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,             # Maksimum epoch
    batch_size=16,
    class_weight=class_weight_dict,
    verbose=0,              # Silent training agar tidak penuh outputnya
    callbacks=[EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)]
)

# Simpan Model
model_dl.save('models/model_toxicity_dl.keras')
print("✅ Deep Learning Model Trained & Saved.")

# VISUALIZATION

# 1. Visualisasi Distribusi Kelas Target
plt.figure(figsize=(6, 4))
sns.countplot(x=y)
plt.title('Distribusi Kelas Target (0=Non-Toxic, 1=Toxic)')
plt.savefig('images/viz1_class_distribution.png') # Save
plt.show()

# 2. Visualisasi Heatmap Korelasi (Fitur Terpilih)
plt.figure(figsize=(10, 8))
# Menggabungkan X dan y sementara untuk korelasi
df_corr = X_final.copy()
df_corr['Target'] = y.values
corr = df_corr.corr()
sns.heatmap(corr, cmap='coolwarm', annot=False) # Annot false agar tidak penuh angka
plt.title('Heatmap Korelasi (30 Fitur Terpilih)')
plt.savefig('images/viz2_correlation.png') # Save
plt.show()

# 3. Visualisasi Training History (Loss) untuk Deep Learning
plt.figure(figsize=(8, 4))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Deep Learning: Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.savefig('images/viz3_dl_loss.png') # Save
plt.show()

print("✅ Visualisasi selesai dan disimpan di folder 'images/'.")

# EVALUATION & COMPARISON

# 1. Pastikan Dictionary Model sudah siap
models_dict = {
    'Baseline (LogReg)': model_baseline,
    'Advanced (RF)': model_rf,
    'Deep Learning (MLP)': model_dl
}

# Container hasil
final_metrics = []

print("=== HASIL EVALUASI PADA DATA TEST (15%) ===")

for name, model in models_dict.items():
    print(f"\n>> Mengevaluasi Model: {name}...")

    # 2. Lakukan Prediksi
    if name == 'Deep Learning (MLP)':
        # Output DL adalah probabilitas, ubah jadi 0/1
        y_pred_prob = model.predict(X_test, verbose=0)
        y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    else:
        # Output Sklearn langsung kelas 0/1
        y_pred = model.predict(X_test)

    # 3. Hitung Metrik
    acc = accuracy_score(y_test, y_pred)

    # Gunakan average='weighted' agar F1-Score lebih adil
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    final_metrics.append({
        'Model': name,
        'Accuracy': acc,
        'F1-Score': f1
    })

    # Print Laporan Detail
    print(classification_report(y_test, y_pred, zero_division=0))

# 4. Membuat Tabel DataFrame
df_metrics = pd.DataFrame(final_metrics)

print("\n=== TABEL PERBANDINGAN AKHIR ===")
# --- PERBAIKAN DI SINI ---
# Kita format spesifik per kolom agar kolom 'Model' (Text) tidak error
display(df_metrics.style.format({
    'Accuracy': '{:.4f}',
    'F1-Score': '{:.4f}'
}))

# 5. Visualisasi
plt.figure(figsize=(10, 6))
df_melted = df_metrics.melt(id_vars="Model", var_name="Metric", value_name="Score")

ax = sns.barplot(data=df_melted, x="Model", y="Score", hue="Metric", palette="viridis")
plt.title("Perbandingan Performa Model (Weighted F1)", fontsize=14)
plt.ylim(0, 1.15)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.legend(loc='lower right')

# Label Angka di atas batang
for container in ax.containers:
    ax.bar_label(container, fmt='%.2f', padding=3, fontsize=10)

plt.savefig('images/viz4_model_comparison.png')
plt.show()